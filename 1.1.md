```python
import nltk
nltk.download()
from nltk.book import *
```
concordance: shows every occurence of a given word
```python
text1.concordance("monstrous")
```
similar: the other words that shows in a similar range of contexts
```python
text1.similar("monstrous")
```
common_contexts: examine the contexts that are shared by two or more words in documents
```python
text1.common_contexts(["monstrous","very"])
```
dispersion_plot:shows the location of aa word within the entire text
```python
text4.dispersion_plot(["citizens","democracy","freedom","duties","America","liberty"])
```
len(): the total tokens within a text
```python
len(text3)
```
set(): the number of distinct tokens
```python
set(text3)
sorted(set(text(3))
```
count(): how often a word occurs in a text
```python
text3.count('smote')
```
**lexical_diversity**: len(text) / len(set(text)), dedicate how many times that each word has been used in average

**percentage**: 100 * text.count('word') / len(text), dedicate the percentage of a word within a text

**FreqDist()**: frequency distribution for each vocabulary item in the text

```python
fdist1 = FreqDist(text1)
print(fdist1)
fdist1.most_common(50)                                      #List the 5o most frequent words of the text
fdist1['whale']                                             #List how many times that the word 'whale' appears
fdist1.plot(50, cumulative = True)                          #Produce a cumulative frequency graph for the 50 most frequently used words
fdist1.hapaxes()                                            #List the words that occurs only once

sorted(w for w in set(text5) if len(w) > 7 and fdist5[w] > 7) #find the long words that occurs more than one times in the text
```
perhaps these will be more characteristic and informative. However, after we use the following code for text4

```python
V = set(text4)
long_words = [w for w in V if len(w) > 15]
sorted(long_words)
```

The result shows that there are a lot of long meaningless words like: yuuuuuuuuuuuummmmmmmmmmmm.  Well, these very long words are often hapaxes (i.e., unique) and perhaps it would be better to find frequently occurring long words.This seems promising since it eliminates frequent short words (e.g., the) and infrequent long words (e.g. antiphilosophists). Here are all words from the chat corpus that are longer than seven characters, that occur more than seven times:

```python
fdist5 = FreqDist(text5)
sorted(w for w in set(text5) if len(w) > 7 and fdist5[w] > 7)
>>['#14-19teens', '#talkcity_adults', '((((((((((', '........', 'Question',
'actually', 'anything', 'computer', 'cute.-ass', 'everyone', 'football',
'innocent', 'listening', 'remember', 'seriously', 'something', 'together',
'tomorrow', 'watching']

```
The results only shows the long words that appeared more than 1 times, but how about we want to know the exactly number and the cummulative frequncy diagram of these relatively-informative words?

```python
def remove(fdist):                    #build a remove function that remove the long words that appeared  less than seventh
...     if len(fdist) > 0:
...             for i,j in fdist.items():
...                     if j < 7:
...                             fdist.pop(i)
...                             return remove(fdist)
...     else:
...             return fdist
fdist5 = FreqDist(for w in text5 if len(w) > 7)             #create a FreqDist that only contains the word lenth more than 7
remove(fdist5)
fdist5.plot(cumulative=True)
```
Diagram:
![alt text][logo]
[logo]:../master/


## Collocation:
A collocation is a sequence of words that occur together usually often. E.g. (red wine)
